# Flat transformer v2 — MLP input projection
#
# nanochat-style architecture: RMSNorm, ReLU², zero-init projections,
# QK norm, per-layer residual scaling + x0 skip connection.
#
# input_mode=mlp: Linear(52→128) → GELU → Linear(128→128)
# Learns nonlinear feature interactions before transformer.
#
# Usage:
#   CUDA_VISIBLE_DEVICES=1 uv run python scripts/train_flat.py --config configs/train_flat_v2_mlp.yaml

model:
  version: v2
  input_mode: mlp
  max_pulses_per_dom: 16
  d_model: 128
  max_doms: 128
  num_heads: 8
  num_layers: 4
  hidden_dim: 256
  head_hidden_dim: 128
  dropout: 0.1

training:
  epochs: 10
  batch_size: 256
  lr: 1e-4
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clip: 1.0
  use_amp: true

data:
  max_events: 10000000
  val_events: 50000
  num_workers: 4
  geometry_path: /groups/pheno/inar/icecube_kaggle/sensor_geometry_normalized.csv

wandb:
  enabled: true
  project: iceaggr
  name: null
  tags:
    - flat-transformer-v2
    - input-mlp

checkpoint:
  dir: checkpoints
  save_every: 5
  resume: null
