# Baseline Experiment Configuration v1
# Full configuration for training the hierarchical DOM model

# Model configuration - optimized for training
model:
  embed_dim: 64
  max_doms: 128
  pulse_hidden_dims: [64, 64]
  dom_encoder_type: pooling
  pool_method: mean  # mean pooling (not mean_max which has issues)
  event_num_heads: 4  # smaller transformer for faster training
  event_num_layers: 2  # 2 layers instead of 4
  event_hidden_dim: 256
  head_hidden_dim: 128
  dropout: 0.1

# Training configuration
training:
  # Optimization
  lr: 1e-3  # higher LR with gradient clipping
  max_lr: 3e-3
  weight_decay: 0.01

  # Batch and epochs
  batch_size: 256  # larger batch for better GPU utilization
  max_epochs: 100

  # Precision and efficiency
  precision: 16-mixed  # Use AMP for RTX 3090
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

  # Validation
  val_check_interval: 1.0  # Validate every epoch

  # Checkpointing
  save_top_k: 3
  monitor: val/loss
  mode: min

# Data configuration
data:
  # Paths (relative to data root in data_config.yaml)
  geometry_path: /groups/pheno/inar/icecube_kaggle/sensor_geometry_normalized.csv

  # DataLoader settings
  num_workers: 4
  pin_memory: true

  # Event limits (null for all data)
  max_train_events: null
  max_val_events: 50000  # Use 50k events for validation

# Logging
logging:
  project: iceaggr
  name: hierarchical-dom-baseline-v1
  tags:
    - baseline
    - dom-pooling
  log_every_n_steps: 50

# Hardware
hardware:
  accelerator: gpu
  devices: 1
